# Automated Data Pipeline (Terraform + LocalStack + Docker + GitHub Actions)

## Project Overview

This project demonstrates an **end-to-end automated data pipeline** using modern Data Engineering tools and best practices. The pipeline provisions cloud infrastructure using **Terraform**, simulates AWS services locally using **LocalStack**, processes data via a **Dockerized ETL job**, and is fully validated through **GitHub Actions CI**.

The goal of this project is to show how production-grade data pipelines can be built, tested, and validated automatically without relying on real cloud resources.

---

##  Architecture

**Workflow:**

1. GitHub Actions CI triggers on every push to `main`
2. LocalStack starts (AWS S3 emulation)
3. Terraform provisions an S3 bucket
4. Input CSV is uploaded to S3
5. Dockerized ETL job processes data
6. Output is written back to S3
7. CI verifies processed output

---

##  Tech Stack

* **Terraform** â€“ Infrastructure as Code (IaC)
* **AWS S3 (LocalStack)** â€“ Cloud service emulation
* **Docker** â€“ Containerized ETL processing
* **Python** â€“ ETL logic
* **GitHub Actions** â€“ CI/CD automation
* **AWS CLI** â€“ Resource interaction & validation

---

##  Project Structure

```
automated-data-pipeline/
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci.yml                # CI pipeline definition
â”‚
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf               # S3 bucket resource
â”‚   â”œâ”€â”€ provider.tf           # AWS provider (LocalStack)
â”‚   â”œâ”€â”€ variables.tf          # Terraform variables
â”‚   â””â”€â”€ output.tf             # Terraform outputs
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ etl.py                # ETL script
â”‚   â””â”€â”€ Dockerfile            # ETL Docker image
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/input.csv         # Input dataset
â”‚
â”œâ”€â”€ docker-compose.yml        # Local development stack
â””â”€â”€ README.md
```

---

## âš™ï¸ Local Execution (Before CI)

### 1ï¸ Start LocalStack

```bash
docker-compose up -d localstack
```

Verify LocalStack:

```bash
curl http://localhost:4566/_localstack/health
```

---

### 2ï¸ Run Terraform

```bash
cd terraform
terraform init
terraform apply -auto-approve
```

---

### 3ï¸ Upload Input Data

```bash
aws --endpoint-url=http://localhost:4566 s3 cp data/raw/input.csv \
  s3://localstack-etl-bucket/raw/input.csv
```

---

### 4ï¸ Run ETL Job

```bash
docker build -t etl-job ./etl

docker run --rm \
  --network host \
  -e AWS_ACCESS_KEY_ID=test \
  -e AWS_SECRET_ACCESS_KEY=test \
  -e AWS_DEFAULT_REGION=us-east-1 \
  -e AWS_ENDPOINT_URL=http://localhost:4566 \
  -e S3_BUCKET_NAME=localstack-etl-bucket \
  etl-job
```

---

### 5ï¸ Verify Output

```bash
aws --endpoint-url=http://localhost:4566 s3 ls \
  s3://localstack-etl-bucket/processed/
```

Expected output:

```
output.csv
```

---

##  CI/CD Pipeline

* Triggered on every push to `main`
* Runs the **full pipeline automatically**
* Fails if:

  * Terraform provisioning fails
  * ETL job fails
  * Output is missing in S3

This ensures **reproducibility, reliability, and correctness**.

---

##  Key Learnings

* Infrastructure as Code with Terraform
* Cloud service emulation using LocalStack
* Docker networking in CI environments
* Debugging CI/CD failures
* Safe Git practices (ignoring provider binaries)

---

## ğŸ“ˆ Future Improvements

* Add data validation tests
* Add CI status badge
* Add CloudWatch / logging simulation
* Parameterize environments (dev/prod)

---

## ğŸ‘¤ Author

**Dhamini Kathula**


 If you found this project useful, feel free to star the repository!
